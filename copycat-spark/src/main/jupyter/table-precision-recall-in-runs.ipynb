{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 util/precision-recall-in-top-1000.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18 = pd.read_json('../../../tmp/web-track-18-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19 = pd.read_json('../../../tmp/web-track-19-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = pd.read_json('../../../tmp/web-track-20-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21 = pd.read_json('../../../tmp/web-track-21-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22 = pd.read_json('../../../tmp/web-track-22-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_23 = pd.read_json('../../../tmp/web-track-23-precision-recall.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision/Recall with S3 score as ground-truth (small cw09 sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Approach</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Top1000</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Relevant@Top1000</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Irrelevant@Top1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CopyCat</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Url Classes</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimHash(1-grams)</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SimHash(3+5-grams)</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TextProfile</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MD5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Approach   Top1000        Relevant@Top1000         \\\n",
       "                      Precision Recall        Precision Recall   \n",
       "0             CopyCat     0.926  0.361            0.994  0.540   \n",
       "1         Url Classes     0.902  0.079            0.986  0.166   \n",
       "2    SimHash(1-grams)     0.749  0.803            0.799  0.890   \n",
       "3  SimHash(3+5-grams)     0.950  0.327            0.998  0.489   \n",
       "4         TextProfile     0.977  0.145            1.000  0.352   \n",
       "5                 MD5     1.000  0.092            1.000  0.307   \n",
       "\n",
       "  Irrelevant@Top1000         \n",
       "           Precision Recall  \n",
       "0              0.870  0.339  \n",
       "1              0.794  0.107  \n",
       "2              0.758  0.902  \n",
       "3              0.927  0.285  \n",
       "4              0.995  0.084  \n",
       "5              1.000  0.040  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_score(df, approach):\n",
    "    from sklearn.metrics import precision_score\n",
    "    return \"{:0.3f}\".format(precision_score(y_true=df['near-duplicate'], y_pred=df[approach]))\n",
    "\n",
    "def recall_score(df, approach):\n",
    "    from sklearn.metrics import recall_score\n",
    "    return \"{:0.3f}\".format(recall_score(y_true=df['near-duplicate'], y_pred=df[approach]))\n",
    "\n",
    "def table_row(df, approach, approach_display_name):\n",
    "    ret = {'Approach': approach_display_name}\n",
    "\n",
    "    for doc_count in [1000]:\n",
    "        df_current_count = df[df['docs'] == doc_count]\n",
    "        doc_count = str(doc_count)\n",
    "        df_relevant = df_current_count[(df_current_count['judged']) & (df_current_count['relevant'])]\n",
    "        df_irrelevant = df_current_count[(df_current_count['judged']) & (~df_current_count['relevant'])]\n",
    "\n",
    "        ret['Precision (Top' + doc_count + ')'] = precision_score(df_current_count, approach)\n",
    "        ret['Recall (Top' + doc_count + ')'] = recall_score(df_current_count, approach)\n",
    "        ret['Precision (Relevant@Top' + doc_count + ')'] = precision_score(df_relevant, approach)\n",
    "        ret['Recall (Relevant@Top' + doc_count + ')'] = recall_score(df_relevant, approach)\n",
    "        ret['Precision (Irrelevant@Top' + doc_count + ')'] = precision_score(df_irrelevant, approach)\n",
    "        ret['Recall (Irrelevant@Top' + doc_count + ')'] = recall_score(df_irrelevant, approach)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def report_table(df):\n",
    "    rows = []\n",
    "    for approach, approach_display_name in [('copy-cat-tp', 'CopyCat'), ('url-simhash', 'Url Classes'), ('simhash(1-grams)', 'SimHash(1-grams)'), ('simhash(3+5-grams)', 'SimHash(3+5-grams)'), ('text-profile', 'TextProfile') , ('md5', 'MD5')]:\n",
    "        rows += [table_row(df, approach, approach_display_name)]\n",
    "    ret = pd.DataFrame(rows)\n",
    "    ret.set_index('Approach', inplace=True)\n",
    "    ret.columns = pd.MultiIndex.from_tuples([\n",
    "        \n",
    "        ('Top1000', 'Precision'), ('Top1000', 'Recall'),\n",
    "        ('Relevant@Top1000', 'Precision'), ('Relevant@Top1000', 'Recall'),\n",
    "        ('Irrelevant@Top1000', 'Precision'), ('Irrelevant@Top1000', 'Recall'),\n",
    "    ])\n",
    "\n",
    "    return ret.reset_index()\n",
    "\n",
    "print('Precision/Recall with S3 score as ground-truth (small cw09 sample):')\n",
    "df = pd.concat([df_18, df_19, df_20, df_21, df_22, df_23])\n",
    "df['url-simhash'] = df['simhash(1-grams)'] & df['url']\n",
    "df['docs'] = 1000\n",
    "df = report_table(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{3pt}%\n",
      "\\caption{TBD. {\\color{red}Make table consume full width.}}\n",
      "\\label{table-precision-recall-in-runs}\n",
      "\\begin{tabular}{@{}lcccccc@{}}\n",
      "\\toprule\n",
      "{\\bfseries Method} & \\multicolumn{2}{c@{}}{\\bfseries Top~1000} & \\multicolumn{2}{c@{}}{\\bfseries Relevant@Top~1000} & \\multicolumn{2}{c@{}}{\\bfseries Irrelevant@Top~1000} \\\\\n",
      "\n",
      "\\cmidrule(l){2-3}\n",
      "\\cmidrule(l){4-5}\n",
      "\\cmidrule(l){6-7}\n",
      "\n",
      "& Prec. & Rec.  & Prec. & Rec. & Prec. & Rec. \\\\\n",
      "\n",
      "\\midrule\n",
      "\n",
      "Crawl & 0.950 & 0.327 & 0.998 & 0.489 & 0.927 & 0.285 \\\\\n",
      "\n",
      "Classes & 0.902 & 0.079 & 0.986 & 0.166 & 0.794 & 0.107 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\resource & 0.926 & 0.361 & 0.994 & 0.540 & 0.870 & 0.339 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(v):\n",
    "    return v\n",
    "\n",
    "def row(name):\n",
    "    r = df[df['Approach'] == name].iloc[0]\n",
    "    return '& ' + f(r[('Top1000', 'Precision')]) + ' & ' + f(r[('Top1000', 'Recall')]) + ' & ' + \\\n",
    "           f(r[('Relevant@Top1000', 'Precision')]) + ' & ' + f(r[('Relevant@Top1000', 'Recall')]) +' & ' + \\\n",
    "           f(r[('Irrelevant@Top1000', 'Precision')]) + ' & ' + f(r[('Irrelevant@Top1000', 'Recall')]) + ' \\\\\\\\'\n",
    "\n",
    "def table():\n",
    "    return \"\"\"\n",
    "\\\\begin{table}\n",
    "\\\\centering\n",
    "\\\\small\n",
    "\\\\setlength{\\\\tabcolsep}{3pt}%\n",
    "\\\\caption{TBD. {\\\\color{red}Make table consume full width.}}\n",
    "\\\\label{table-precision-recall-in-runs}\n",
    "\\\\begin{tabular}{@{}lcccccc@{}}\n",
    "\\\\toprule\n",
    "{\\\\bfseries Method} & \\\\multicolumn{2}{c@{}}{\\\\bfseries Top~1000} & \\\\multicolumn{2}{c@{}}{\\\\bfseries Relevant@Top~1000} & \\\\multicolumn{2}{c@{}}{\\\\bfseries Irrelevant@Top~1000} \\\\\\\\\n",
    "\n",
    "\\\\cmidrule(l){2-3}\n",
    "\\\\cmidrule(l){4-5}\n",
    "\\\\cmidrule(l){6-7}\n",
    "\n",
    "& Prec. & Rec.  & Prec. & Rec. & Prec. & Rec. \\\\\\\\\n",
    "\n",
    "\\\\midrule\n",
    "\n",
    "Crawl \"\"\" +  row('SimHash(3+5-grams)') + \"\"\"\n",
    "\n",
    "Classes \"\"\" +  row('Url Classes') + \"\"\"\n",
    "\\\\midrule\n",
    "\n",
    "\\\\resource \"\"\" +  row('CopyCat') + \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "print(table())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
